{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# DSPy as a Service - HotPotQA Optimization with GEPA\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Submit an optimization job using the GEPA optimizer on HotPotQA dataset\n",
    "2. Monitor progress in real-time\n",
    "3. Retrieve and use the optimized program\n",
    "\n",
    "HotPotQA is a multi-hop question answering dataset that requires reasoning over multiple documents.\n",
    "GEPA (Reflective Prompt Evolution) can improve accuracy from ~24% to ~51% on this task."
   ]
  },
  {
   "cell_type": "code",
   "id": "9bfa9335d04b957f",
   "metadata": {},
   "source": "# TODO: On-premise - Update pip index URL to local artifactory\n# Example: !pip install -q dspy requests --index-url https://artifactory.your-company.com/api/pypi/pypi-remote/simple\n!pip install -q dspy requests",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "11219e82627f31e2",
   "metadata": {},
   "source": [
    "import base64\n",
    "import inspect\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import textwrap\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import dspy\n",
    "import requests"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# TODO: On-premise - Update BASE_URL to your internal DSPy service endpoint\nBASE_URL = os.getenv(\"DSPY_SERVICE_URL\", \"http://localhost:8000\")\n\n# API key from environment variable (required)\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nif not OPENAI_API_KEY:\n    raise ValueError(\"OPENAI_API_KEY environment variable is required\")\n\n# TODO: On-premise - Update model config for local LLM provider\n# Example for local provider:\n#   \"name\": \"ollama/llama3\" or \"local/mistral\"\n#   \"base_url\": \"http://your-local-llm:11434/v1\"\n#   \"extra\": {} (no API key needed)\nMODEL_CONFIG = {\n    \"name\": \"openai/gpt-4o-mini\",\n    \"base_url\": \"https://api.openai.com/v1\",\n    \"model_type\": \"responses\",\n    \"temperature\": 1.0,\n    \"max_tokens\": 20000,\n    \"extra\": {\"api_key\": OPENAI_API_KEY},\n}\n\n# TODO: On-premise - Update dspy.LM config to match MODEL_CONFIG above\ndspy.configure(\n    lm=dspy.LM(\n        \"openai/gpt-4o-mini\",\n        model_type=\"responses\",\n        temperature=1.0,\n        max_tokens=20000,\n        api_key=OPENAI_API_KEY,\n    )\n)",
   "id": "fc8a445363802c03",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d8eaebc1df654267",
   "metadata": {},
   "source": [
    "## API Client\n",
    "\n",
    "A simple client for interacting with the DSPy service."
   ]
  },
  {
   "cell_type": "code",
   "id": "ef23b185e98b8f2",
   "metadata": {},
   "source": "class DSPyServiceClient:\n    \"\"\"Client for the DSPy optimization service.\n    \n    Provides methods to submit jobs, check status, and retrieve results.\n    \n    Args:\n        base_url: Root URL of the DSPy service API.\n    \"\"\"\n    \n    def __init__(self, base_url: str = \"http://localhost:8000\"):\n        self.base_url = base_url.rstrip(\"/\")\n    \n    def health(self) -> dict:\n        \"\"\"Check service health status.\n        \n        Returns:\n            dict: Health status with registered assets.\n        \"\"\"\n        return requests.get(f\"{self.base_url}/health\").json()\n    \n    def submit(self, payload: dict) -> str:\n        \"\"\"Submit an optimization job.\n        \n        Args:\n            payload: Job configuration including module, optimizer, and dataset.\n        \n        Returns:\n            str: The job ID for tracking progress.\n        \n        Raises:\n            requests.HTTPError: If submission fails.\n        \"\"\"\n        resp = requests.post(f\"{self.base_url}/run\", json=payload)\n        resp.raise_for_status()\n        return resp.json()[\"job_id\"]\n    \n    def status(self, job_id: str) -> dict:\n        \"\"\"Get current job status.\n        \n        Args:\n            job_id: The job identifier.\n        \n        Returns:\n            dict: Full job status including progress events and result.\n        \n        Raises:\n            requests.HTTPError: If job not found.\n        \"\"\"\n        resp = requests.get(f\"{self.base_url}/jobs/{job_id}\")\n        resp.raise_for_status()\n        return resp.json()\n    \n    def summary(self, job_id: str) -> dict:\n        \"\"\"Get lightweight job summary.\n        \n        Args:\n            job_id: The job identifier.\n        \n        Returns:\n            dict: Summary with timing and configuration info.\n        \"\"\"\n        return requests.get(f\"{self.base_url}/jobs/{job_id}/summary\").json()\n    \n    def logs(self, job_id: str) -> list:\n        \"\"\"Get job execution logs.\n        \n        Args:\n            job_id: The job identifier.\n        \n        Returns:\n            list: Log entries with timestamp, level, and message.\n        \"\"\"\n        return requests.get(f\"{self.base_url}/jobs/{job_id}/logs\").json()\n    \n    def artifact(self, job_id: str) -> dict:\n        \"\"\"Get the optimized program artifact.\n        \n        Args:\n            job_id: The job identifier.\n        \n        Returns:\n            dict: Artifact with base64-encoded program and metadata.\n        \n        Raises:\n            requests.HTTPError: If job not complete or failed.\n        \"\"\"\n        resp = requests.get(f\"{self.base_url}/jobs/{job_id}/artifact\")\n        resp.raise_for_status()\n        return resp.json()\n    \n    def load_program(self, job_id: str) -> dspy.Module:\n        \"\"\"Load the optimized program from a completed job.\n        \n        Args:\n            job_id: The job identifier.\n        \n        Returns:\n            dspy.Module: The optimized DSPy module ready for inference.\n        \n        Raises:\n            requests.HTTPError: If artifact not available.\n            ValueError: If artifact missing program data.\n        \"\"\"\n        artifact = self.artifact(job_id)\n        pickle_b64 = artifact[\"program_artifact\"][\"program_pickle_base64\"]\n        return pickle.loads(base64.b64decode(pickle_b64))\n\n\nclient = DSPyServiceClient(BASE_URL)\nclient.health()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "monitor-header",
   "metadata": {},
   "source": [
    "## Job Monitor\n",
    "\n",
    "Clean progress monitoring with formatted output."
   ]
  },
  {
   "cell_type": "code",
   "id": "monitor",
   "metadata": {},
   "source": "class JobMonitor:\n    \"\"\"Monitor job progress with formatted console output.\n    \n    Args:\n        client: DSPyServiceClient instance.\n        job_id: The job identifier to monitor.\n    \"\"\"\n    \n    def __init__(self, client: \"DSPyServiceClient\", job_id: str):\n        self.client = client\n        self.job_id = job_id\n        self._printed_events = 0\n        self._printed_logs = 0\n    \n    def poll(self, interval: int = 3, timeout: int = None, verbose: bool = True) -> dict:\n        \"\"\"Poll until job completes.\n        \n        Args:\n            interval: Seconds between status checks.\n            timeout: Maximum seconds to wait (None for unlimited).\n            verbose: Whether to print status updates.\n        \n        Returns:\n            dict: Final job status with result or error.\n        \n        Raises:\n            TimeoutError: If job doesn't complete within timeout (when set).\n        \"\"\"\n        start = time.time()\n        \n        while True:\n            status = self.client.status(self.job_id)\n            elapsed = time.time() - start\n            \n            if verbose:\n                self._print_status(status, elapsed)\n            \n            if status[\"status\"] in {\"success\", \"failed\"}:\n                return status\n            \n            if timeout is not None and elapsed > timeout:\n                raise TimeoutError(\"Job \" + self.job_id + \" timed out after \" + str(timeout) + \"s\")\n            \n            time.sleep(interval)\n    \n    def _print_status(self, status: dict, elapsed: float) -> None:\n        \"\"\"Print formatted status line with new events and logs.\n        \n        Args:\n            status: Current job status dict.\n            elapsed: Seconds since polling started.\n        \"\"\"\n        ts = time.strftime(\"%H:%M:%S\")\n        metrics = status.get(\"latest_metrics\", {})\n        \n        print(\"[\" + ts + \"] \" + status[\"status\"].upper().ljust(12) + \" | elapsed: \" + str(int(elapsed)) + \"s | \" + self._format_metrics(metrics))\n        \n        events = status.get(\"progress_events\", [])\n        for event in events[self._printed_events:]:\n            self._print_event(event)\n        self._printed_events = len(events)\n        \n        logs = status.get(\"logs\", [])\n        for log in logs[self._printed_logs:]:\n            self._print_log(log)\n        self._printed_logs = len(logs)\n    \n    @staticmethod\n    def _format_metrics(metrics: dict) -> str:\n        \"\"\"Format metrics dict for display.\n        \n        Args:\n            metrics: Latest metrics from job status.\n        \n        Returns:\n            str: Formatted metrics string.\n        \"\"\"\n        if not metrics:\n            return \"\"\n        \n        parts = []\n        if \"tqdm_percent\" in metrics:\n            parts.append(str(round(metrics[\"tqdm_percent\"], 1)) + \"%\")\n        if \"tqdm_n\" in metrics and \"tqdm_total\" in metrics:\n            parts.append(str(metrics[\"tqdm_n\"]) + \"/\" + str(metrics[\"tqdm_total\"]))\n        if \"baseline_test_metric\" in metrics:\n            parts.append(\"baseline: \" + str(round(metrics[\"baseline_test_metric\"], 2)))\n        if \"optimized_test_metric\" in metrics:\n            parts.append(\"optimized: \" + str(round(metrics[\"optimized_test_metric\"], 2)))\n        \n        return \" | \".join(parts) if parts else \"\"\n    \n    @staticmethod\n    def _print_event(event: dict) -> None:\n        \"\"\"Print a progress event.\n        \n        Args:\n            event: Progress event dict with name and metrics.\n        \"\"\"\n        name = event.get(\"event\", \"progress\")\n        metrics = event.get(\"metrics\", {})\n        \n        if name == \"optimizer_progress\" and \"tqdm_desc\" in metrics:\n            desc = metrics[\"tqdm_desc\"]\n            pct = metrics.get(\"tqdm_percent\", 0)\n            n = metrics.get(\"tqdm_n\", 0)\n            total = metrics.get(\"tqdm_total\", \"?\")\n            print(\"       \" + str(desc) + \": \" + str(round(pct, 1)) + \"% (\" + str(n) + \"/\" + str(total) + \")\")\n        elif metrics:\n            print(\"       \" + str(name) + \": \" + str(metrics))\n    \n    @staticmethod\n    def _print_log(log: dict) -> None:\n        \"\"\"Print a log entry.\n        \n        Args:\n            log: Log entry dict with level and message.\n        \"\"\"\n        level = log.get(\"level\", \"INFO\")\n        msg = log.get(\"message\", \"\")\n        if msg and level in {\"INFO\", \"WARNING\", \"ERROR\"}:\n            print(\"       [\" + level + \"] \" + msg)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dataset-header",
   "metadata": {},
   "source": [
    "## Dataset & Signature\n",
    "\n",
    "Load the HotPotQA dataset - multi-hop question answering requiring reasoning over multiple facts."
   ]
  },
  {
   "cell_type": "code",
   "id": "dataset",
   "metadata": {},
   "source": "DATA_PATH = Path(\"data/hotpotqa.json\")\nwith open(DATA_PATH) as f:\n    DATASET = json.load(f)\n\nprint(f\"Loaded {len(DATASET)} examples\")\nprint(f\"Sample: {DATASET[0]}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "signature",
   "metadata": {},
   "source": "class HotPotQA(dspy.Signature):\n    \"\"\"Answer multi-hop questions requiring reasoning over multiple facts.\"\"\"\n    question: str = dspy.InputField(desc=\"A question requiring multi-hop reasoning\")\n    answer: str = dspy.OutputField(desc=\"The answer to the question\")\n\n\ndef hotpotqa_metric(gold: dspy.Example, pred: dspy.Prediction, trace=None, pred_name=None, pred_trace=None) -> dspy.Prediction:\n    \"\"\"Score prediction with feedback for GEPA reflection.\n    \n    Args:\n        gold: Ground truth example with expected answer.\n        pred: Model prediction with generated answer.\n        trace: Optional execution trace.\n        pred_name: Optional predictor name.\n        pred_trace: Optional predictor trace.\n    \n    Returns:\n        dspy.Prediction: Contains score (0.0-1.0) and feedback text.\n    \"\"\"\n    expected = (gold.answer or \"\").strip().lower()\n    actual = (pred.answer or \"\").strip().lower()\n    \n    if expected == actual:\n        return dspy.Prediction(score=1.0, feedback=\"Correct answer.\")\n    elif expected in actual or actual in expected:\n        feedback = \"Partial match. Expected '\" + str(gold.answer) + \"', got '\" + str(pred.answer) + \"'.\"\n        return dspy.Prediction(score=0.5, feedback=feedback)\n    else:\n        feedback = \"Incorrect. Expected '\" + str(gold.answer) + \"', got '\" + str(pred.answer) + \"'. Try reasoning step-by-step over multiple facts.\"\n        return dspy.Prediction(score=0.0, feedback=feedback)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5f688280d0c96e61",
   "metadata": {},
   "source": "def serialize_source(obj: Any) -> str:\n    \"\"\"Extract source code from a function or DSPy Signature class.\n    \n    Args:\n        obj: A function or dspy.Signature subclass.\n    \n    Returns:\n        str: Python source code as a string.\n    \n    Raises:\n        RuntimeError: If source cannot be extracted.\n    \"\"\"\n    if isinstance(obj, type) and issubclass(obj, dspy.Signature):\n        doc = obj.__doc__ or \"\"\n        lines = [\n            \"class \" + obj.__name__ + \"(dspy.Signature):\",\n            '    \"\"\"' + doc + '\"\"\"',\n        ]\n        for name, field in obj.model_fields.items():\n            extra = field.json_schema_extra or {}\n            ftype = \"InputField\" if extra.get(\"__dspy_field_type\") == \"input\" else \"OutputField\"\n            desc = extra.get(\"desc\", \"\")\n            lines.append(\"    \" + name + ': str = dspy.' + ftype + '(desc=\"' + desc + '\")')\n        return \"\\n\".join(lines)\n    \n    try:\n        return textwrap.dedent(inspect.getsource(obj)).strip()\n    except (OSError, TypeError) as e:\n        raise RuntimeError(\"Cannot extract source from \" + str(obj)) from e\n\n\nSIGNATURE_CODE = serialize_source(HotPotQA)\nMETRIC_CODE = serialize_source(hotpotqa_metric)\n\nprint(\"Signature:\")\nprint(SIGNATURE_CODE)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "payload-header",
   "metadata": {},
   "source": [
    "## Build Payload\n",
    "\n",
    "Configure GEPA optimizer for prompt evolution on HotPotQA."
   ]
  },
  {
   "cell_type": "code",
   "id": "payload",
   "metadata": {},
   "source": "payload = {\n    \"module_name\": \"dspy.ChainOfThought\",\n    \"signature_code\": SIGNATURE_CODE,\n    \"metric_code\": METRIC_CODE,\n    \"optimizer_name\": \"dspy.GEPA\",\n    \"optimizer_kwargs\": {\n        \"auto\": \"light\",\n        \"num_threads\": 8,\n        \"reflection_minibatch_size\": 3,\n    },\n    \"compile_kwargs\": {},\n    \"dataset\": DATASET,\n    \"column_mapping\": {\n        \"inputs\": {\"question\": \"question\"},\n        \"outputs\": {\"answer\": \"answer\"},\n    },\n    \"split_fractions\": {\"train\": 0.6, \"val\": 0.2, \"test\": 0.2},\n    \"shuffle\": True,\n    \"seed\": 42,\n    \"model_config\": MODEL_CONFIG,\n    \"reflection_model_config\": MODEL_CONFIG,\n}\n\nprint(f\"Module: {payload['module_name']}\")\nprint(f\"Optimizer: {payload['optimizer_name']}\")\nprint(f\"Dataset: {len(payload['dataset'])} examples\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "submit-header",
   "metadata": {},
   "source": [
    "## Submit & Monitor Job"
   ]
  },
  {
   "cell_type": "code",
   "id": "submit",
   "metadata": {},
   "source": [
    "job_id = client.submit(payload)\n",
    "print(f\"Submitted job: {job_id}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "poll",
   "metadata": {},
   "source": "monitor = JobMonitor(client, job_id)\nresult = monitor.poll(interval=3)\n\nprint(\"\\nFinal status: \" + result[\"status\"])",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "results-header",
   "metadata": {},
   "source": [
    "## View Results"
   ]
  },
  {
   "cell_type": "code",
   "id": "results",
   "metadata": {},
   "source": [
    "def print_results(result: dict) -> None:\n",
    "    \"\"\"Print optimization results summary.\n",
    "    \n",
    "    Args:\n",
    "        result: Final job status dict from polling.\n",
    "    \"\"\"\n",
    "    if result[\"status\"] == \"success\":\n",
    "        r = result[\"result\"]\n",
    "        print(f\"Baseline score:  {r.get('baseline_test_metric', 'N/A')}\")\n",
    "        print(f\"Optimized score: {r.get('optimized_test_metric', 'N/A')}\")\n",
    "        print(f\"Runtime: {r.get('runtime_seconds', 0):.1f}s\")\n",
    "    else:\n",
    "        print(f\"Job failed: {result.get('message')}\")\n",
    "\n",
    "\n",
    "print_results(result)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "summary",
   "metadata": {},
   "source": [
    "client.summary(job_id)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "logs",
   "metadata": {},
   "source": [
    "def print_recent_logs(client: \"DSPyServiceClient\", job_id: str, n: int = 5) -> None:\n",
    "    \"\"\"Print the most recent log entries.\n",
    "    \n",
    "    Args:\n",
    "        client: DSPyServiceClient instance.\n",
    "        job_id: The job identifier.\n",
    "        n: Number of recent logs to display.\n",
    "    \n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    logs = client.logs(job_id)\n",
    "    print(f\"Total log entries: {len(logs)}\")\n",
    "    for log in logs[-n:]:\n",
    "        print(f\"  [{log['level']}] {log['message'][:80]}\")\n",
    "\n",
    "\n",
    "print_recent_logs(client, job_id)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "load-header",
   "metadata": {},
   "source": [
    "## Load & Test Optimized Program"
   ]
  },
  {
   "cell_type": "code",
   "id": "load",
   "metadata": {},
   "source": "program = client.load_program(job_id)\nprint(f\"Loaded program: {type(program).__name__}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "test",
   "metadata": {},
   "source": [
    "def test_program(program: dspy.Module, questions: list[str]) -> None:\n",
    "    \"\"\"Run test questions through the optimized program.\n",
    "    \n",
    "    Args:\n",
    "        program: The optimized DSPy module.\n",
    "        questions: List of test questions to run.\n",
    "    \"\"\"\n",
    "    for q in questions:\n",
    "        response = program(question=q)\n",
    "        print(f\"Q: {q}\")\n",
    "        print(f\"A: {response.answer}\\n\")\n",
    "\n",
    "\n",
    "test_questions = [\n",
    "    \"Were Scott Derrickson and Ed Wood of the same nationality?\",\n",
    "    \"What government position was held by the woman who portrayed Portia in The Merchant of Venice?\",\n",
    "    \"What is the name of the fight song of the university whose main campus is in Lawrence, Kansas?\",\n",
    "]\n",
    "\n",
    "test_program(program, test_questions)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "history",
   "metadata": {},
   "source": [
    "dspy.inspect_history(n=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "eebdbdb211e08f40",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}