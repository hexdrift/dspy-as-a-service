{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# DSPy as a Service - GSM8K Math Optimization with GEPA\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Submit an optimization job using the GEPA optimizer on GSM8K math dataset\n",
    "2. Monitor progress in real-time\n",
    "3. Retrieve and use the optimized program\n",
    "\n",
    "GSM8K is a dataset of grade school math word problems requiring multi-step reasoning.\n",
    "GEPA (Reflective Prompt Evolution) can achieve up to 93% accuracy on math benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "id": "install",
   "metadata": {},
   "source": "# TODO: On-premise - Update pip index URL to local artifactory\n# Example: !pip install -q dspy requests --index-url https://artifactory.your-company.com/api/pypi/pypi-remote/simple\n!pip install -q dspy requests",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "imports",
   "metadata": {},
   "source": [
    "import base64\n",
    "import inspect\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import textwrap\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import dspy\n",
    "import requests"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "id": "config",
   "metadata": {},
   "source": "# TODO: On-premise - Update BASE_URL to your internal DSPy service endpoint\nBASE_URL = os.getenv(\"DSPY_SERVICE_URL\", \"http://localhost:8000\")\n\n# API key from environment variable (required)\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nif not OPENAI_API_KEY:\n    raise ValueError(\"OPENAI_API_KEY environment variable is required\")\n\n# TODO: On-premise - Update model config for local LLM provider\n# Example for local provider:\n#   \"name\": \"ollama/llama3\" or \"local/mistral\"\n#   \"base_url\": \"http://your-local-llm:11434/v1\"\n#   \"extra\": {} (no API key needed)\nMODEL_CONFIG = {\n    \"name\": \"openai/gpt-4o-mini\",\n    \"base_url\": \"https://api.openai.com/v1\",\n    \"model_type\": \"responses\",\n    \"temperature\": 1.0,\n    \"max_tokens\": 20000,\n    \"extra\": {\"api_key\": OPENAI_API_KEY},\n}\n\n# TODO: On-premise - Update dspy.LM config to match MODEL_CONFIG above\ndspy.configure(\n    lm=dspy.LM(\n        \"openai/gpt-4o-mini\",\n        model_type=\"responses\",\n        temperature=1.0,\n        max_tokens=20000,\n        api_key=OPENAI_API_KEY,\n    )\n)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "client-header",
   "metadata": {},
   "source": [
    "## API Client\n",
    "\n",
    "A simple client for interacting with the DSPy service."
   ]
  },
  {
   "cell_type": "code",
   "id": "client",
   "metadata": {},
   "source": [
    "class DSPyServiceClient:\n",
    "    \"\"\"Client for the DSPy optimization service.\n",
    "    \n",
    "    Provides methods to submit jobs, check status, and retrieve results.\n",
    "    \n",
    "    Args:\n",
    "        base_url: Root URL of the DSPy service API.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_url: str = \"http://localhost:8000\"):\n",
    "        self.base_url = base_url.rstrip(\"/\")\n",
    "    \n",
    "    def health(self) -> dict:\n",
    "        \"\"\"Check service health status.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Health status with registered assets.\n",
    "        \"\"\"\n",
    "        return requests.get(f\"{self.base_url}/health\").json()\n",
    "    \n",
    "    def submit(self, payload: dict) -> str:\n",
    "        \"\"\"Submit an optimization job.\n",
    "        \n",
    "        Args:\n",
    "            payload: Job configuration including module, optimizer, and dataset.\n",
    "        \n",
    "        Returns:\n",
    "            str: The job ID for tracking progress.\n",
    "        \n",
    "        Raises:\n",
    "            requests.HTTPError: If submission fails.\n",
    "        \"\"\"\n",
    "        resp = requests.post(f\"{self.base_url}/run\", json=payload)\n",
    "        resp.raise_for_status()\n",
    "        return resp.json()[\"job_id\"]\n",
    "    \n",
    "    def status(self, job_id: str) -> dict:\n",
    "        \"\"\"Get current job status.\n",
    "        \n",
    "        Args:\n",
    "            job_id: The job identifier.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Full job status including progress events and result.\n",
    "        \n",
    "        Raises:\n",
    "            requests.HTTPError: If job not found.\n",
    "        \"\"\"\n",
    "        resp = requests.get(f\"{self.base_url}/jobs/{job_id}\")\n",
    "        resp.raise_for_status()\n",
    "        return resp.json()\n",
    "    \n",
    "    def summary(self, job_id: str) -> dict:\n",
    "        \"\"\"Get lightweight job summary.\n",
    "        \n",
    "        Args:\n",
    "            job_id: The job identifier.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Summary with timing and configuration info.\n",
    "        \"\"\"\n",
    "        return requests.get(f\"{self.base_url}/jobs/{job_id}/summary\").json()\n",
    "    \n",
    "    def logs(self, job_id: str) -> list:\n",
    "        \"\"\"Get job execution logs.\n",
    "        \n",
    "        Args:\n",
    "            job_id: The job identifier.\n",
    "        \n",
    "        Returns:\n",
    "            list: Log entries with timestamp, level, and message.\n",
    "        \"\"\"\n",
    "        return requests.get(f\"{self.base_url}/jobs/{job_id}/logs\").json()\n",
    "    \n",
    "    def artifact(self, job_id: str) -> dict:\n",
    "        \"\"\"Get the optimized program artifact.\n",
    "        \n",
    "        Args:\n",
    "            job_id: The job identifier.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Artifact with base64-encoded program and metadata.\n",
    "        \n",
    "        Raises:\n",
    "            requests.HTTPError: If job not complete or failed.\n",
    "        \"\"\"\n",
    "        resp = requests.get(f\"{self.base_url}/jobs/{job_id}/artifact\")\n",
    "        resp.raise_for_status()\n",
    "        return resp.json()\n",
    "    \n",
    "    def load_program(self, job_id: str) -> dspy.Module:\n",
    "        \"\"\"Load the optimized program from a completed job.\n",
    "        \n",
    "        Args:\n",
    "            job_id: The job identifier.\n",
    "        \n",
    "        Returns:\n",
    "            dspy.Module: The optimized DSPy module ready for inference.\n",
    "        \n",
    "        Raises:\n",
    "            requests.HTTPError: If artifact not available.\n",
    "            ValueError: If artifact missing program data.\n",
    "        \"\"\"\n",
    "        artifact = self.artifact(job_id)\n",
    "        pickle_b64 = artifact[\"program_artifact\"][\"program_pickle_base64\"]\n",
    "        return pickle.loads(base64.b64decode(pickle_b64))\n",
    "\n",
    "\n",
    "    def submit_df(self, df: \"pd.DataFrame\", column_mapping: dict, **kwargs) -> str:\n",
    "        \"\"\"Submit an optimization job using a pandas DataFrame.\n",
    "        \n",
    "        Automatically converts the DataFrame to the required format and validates\n",
    "        that all columns specified in column_mapping exist in the DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            df: pandas DataFrame containing the dataset.\n",
    "            column_mapping: Column mapping with 'inputs' and 'outputs' dicts.\n",
    "                Example: {\"inputs\": {\"question\": \"question_col\"}, \n",
    "                         \"outputs\": {\"answer\": \"answer_col\"}}\n",
    "            **kwargs: All other parameters for the optimization job \n",
    "                (module_name, optimizer_name, signature_code, metric_code, \n",
    "                 model_config, etc.).\n",
    "        \n",
    "        Returns:\n",
    "            str: The job ID for tracking progress.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If required columns are missing from the DataFrame.\n",
    "            TypeError: If df is not a DataFrame.\n",
    "            ImportError: If pandas is not installed.\n",
    "        \"\"\"\n",
    "        # Import and type check\n",
    "        try:\n",
    "            import pandas as pd\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"pandas is required to use submit_df(). \"\n",
    "                \"Install it with: pip install pandas\"\n",
    "            )\n",
    "        \n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise TypeError(f\"Expected pandas DataFrame, got {type(df).__name__}\")\n",
    "        \n",
    "        # Validate non-empty\n",
    "        if df.empty:\n",
    "            raise ValueError(\"DataFrame must contain at least one row\")\n",
    "        \n",
    "        # Extract required columns from mapping\n",
    "        required_columns = set()\n",
    "        if \"inputs\" in column_mapping:\n",
    "            required_columns.update(column_mapping[\"inputs\"].values())\n",
    "        if \"outputs\" in column_mapping:\n",
    "            required_columns.update(column_mapping[\"outputs\"].values())\n",
    "        \n",
    "        # Check column existence\n",
    "        df_columns = set(df.columns)\n",
    "        missing_columns = required_columns - df_columns\n",
    "        if missing_columns:\n",
    "            raise ValueError(\n",
    "                f\"DataFrame missing required columns: {sorted(missing_columns)}. \"\n",
    "                f\"Available columns: {sorted(df_columns)}\"\n",
    "            )\n",
    "        \n",
    "        # Convert to records format\n",
    "        dataset = df.to_dict('records')\n",
    "        \n",
    "        # Build payload and delegate to submit()\n",
    "        payload = {\"dataset\": dataset, \"column_mapping\": column_mapping, **kwargs}\n",
    "        return self.submit(payload)\n",
    "    \n",
    "    def submit_df_simple(self, df: \"pd.DataFrame\", input_cols: list, output_cols: list, **kwargs) -> str:\n",
    "        \"\"\"Submit DataFrame with automatic 1:1 column mapping.\n",
    "        \n",
    "        Convenience method for when DataFrame columns match signature field names exactly.\n",
    "        \n",
    "        Args:\n",
    "            df: pandas DataFrame containing the dataset.\n",
    "            input_cols: List of column names to use as inputs.\n",
    "            output_cols: List of column names to use as outputs.\n",
    "            **kwargs: All other optimization job parameters.\n",
    "        \n",
    "        Returns:\n",
    "            str: The job ID for tracking progress.\n",
    "        \n",
    "        Example:\n",
    "            job_id = client.submit_df_simple(\n",
    "                df=my_df,\n",
    "                input_cols=[\"question\"],\n",
    "                output_cols=[\"answer\"],\n",
    "                module_name=\"dspy.ChainOfThought\",\n",
    "                optimizer_name=\"dspy.GEPA\",\n",
    "                signature_code=SIGNATURE_CODE,\n",
    "                metric_code=METRIC_CODE,\n",
    "                model_config=MODEL_CONFIG\n",
    "            )\n",
    "        \"\"\"\n",
    "        column_mapping = {\n",
    "            \"inputs\": {col: col for col in input_cols},\n",
    "            \"outputs\": {col: col for col in output_cols}\n",
    "        }\n",
    "        return self.submit_df(df, column_mapping, **kwargs)\n",
    "    \n",
    "\n",
    "client = DSPyServiceClient(BASE_URL)\n",
    "client.health()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "monitor-header",
   "metadata": {},
   "source": [
    "## Job Monitor\n",
    "\n",
    "Clean progress monitoring with formatted output."
   ]
  },
  {
   "cell_type": "code",
   "id": "monitor",
   "metadata": {},
   "source": "class JobMonitor:\n    \"\"\"Monitor job progress with formatted console output.\n    \n    Args:\n        client: DSPyServiceClient instance.\n        job_id: The job identifier to monitor.\n    \"\"\"\n    \n    def __init__(self, client: \"DSPyServiceClient\", job_id: str):\n        self.client = client\n        self.job_id = job_id\n        self._printed_events = 0\n        self._printed_logs = 0\n    \n    def poll(self, interval: int = 3, timeout: int = None, verbose: bool = True) -> dict:\n        \"\"\"Poll until job completes.\n        \n        Args:\n            interval: Seconds between status checks.\n            timeout: Maximum seconds to wait (None for unlimited).\n            verbose: Whether to print status updates.\n        \n        Returns:\n            dict: Final job status with result or error.\n        \n        Raises:\n            TimeoutError: If job doesn't complete within timeout (when set).\n        \"\"\"\n        start = time.time()\n        \n        while True:\n            status = self.client.status(self.job_id)\n            elapsed = time.time() - start\n            \n            if verbose:\n                self._print_status(status, elapsed)\n            \n            if status[\"status\"] in {\"success\", \"failed\"}:\n                return status\n            \n            if timeout is not None and elapsed > timeout:\n                raise TimeoutError(\"Job \" + self.job_id + \" timed out after \" + str(timeout) + \"s\")\n            \n            time.sleep(interval)\n    \n    def _print_status(self, status: dict, elapsed: float) -> None:\n        \"\"\"Print formatted status line with new events and logs.\n        \n        Args:\n            status: Current job status dict.\n            elapsed: Seconds since polling started.\n        \"\"\"\n        ts = time.strftime(\"%H:%M:%S\")\n        metrics = status.get(\"latest_metrics\", {})\n        \n        print(\"[\" + ts + \"] \" + status[\"status\"].upper().ljust(12) + \" | elapsed: \" + str(int(elapsed)) + \"s | \" + self._format_metrics(metrics))\n        \n        events = status.get(\"progress_events\", [])\n        for event in events[self._printed_events:]:\n            self._print_event(event)\n        self._printed_events = len(events)\n        \n        logs = status.get(\"logs\", [])\n        for log in logs[self._printed_logs:]:\n            self._print_log(log)\n        self._printed_logs = len(logs)\n    \n    @staticmethod\n    def _format_metrics(metrics: dict) -> str:\n        \"\"\"Format metrics dict for display.\n        \n        Args:\n            metrics: Latest metrics from job status.\n        \n        Returns:\n            str: Formatted metrics string.\n        \"\"\"\n        if not metrics:\n            return \"\"\n        \n        parts = []\n        if \"tqdm_percent\" in metrics:\n            parts.append(str(round(metrics[\"tqdm_percent\"], 1)) + \"%\")\n        if \"tqdm_n\" in metrics and \"tqdm_total\" in metrics:\n            parts.append(str(metrics[\"tqdm_n\"]) + \"/\" + str(metrics[\"tqdm_total\"]))\n        if \"tqdm_desc\" in metrics:\n            parts.append(metrics[\"tqdm_desc\"])\n        if \"baseline_test_metric\" in metrics:\n            parts.append(\"baseline: \" + str(round(metrics[\"baseline_test_metric\"], 2)))\n        if \"optimized_test_metric\" in metrics:\n            parts.append(\"optimized: \" + str(round(metrics[\"optimized_test_metric\"], 2)))\n        \n        return \" | \".join(parts) if parts else \"\"\n    \n    @staticmethod\n    def _print_event(event: dict) -> None:\n        \"\"\"Print a progress event.\n        \n        Args:\n            event: Progress event dict with name and metrics.\n        \"\"\"\n        name = event.get(\"event\", \"progress\")\n        metrics = event.get(\"metrics\", {})\n        \n        if name == \"optimizer_progress\" and \"tqdm_desc\" in metrics:\n            desc = metrics[\"tqdm_desc\"]\n            pct = metrics.get(\"tqdm_percent\", 0)\n            n = metrics.get(\"tqdm_n\", 0)\n            total = metrics.get(\"tqdm_total\", \"?\")\n            print(\"       \" + str(desc) + \": \" + str(round(pct, 1)) + \"% (\" + str(n) + \"/\" + str(total) + \")\")\n        elif metrics:\n            print(\"       \" + str(name) + \": \" + str(metrics))\n    \n    @staticmethod\n    def _print_log(log: dict) -> None:\n        \"\"\"Print a log entry.\n        \n        Args:\n            log: Log entry dict with level and message.\n        \"\"\"\n        level = log.get(\"level\", \"INFO\")\n        msg = log.get(\"message\", \"\")\n        if msg and level in {\"INFO\", \"WARNING\", \"ERROR\"}:\n            print(\"       [\" + level + \"] \" + msg)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dataset-header",
   "metadata": {},
   "source": [
    "## Dataset & Signature\n",
    "\n",
    "Load the GSM8K dataset - grade school math word problems requiring multi-step reasoning."
   ]
  },
  {
   "cell_type": "code",
   "id": "dataset",
   "metadata": {},
   "source": [
    "DATA_PATH = Path(\"data/gsm8k.json\")\n",
    "with open(DATA_PATH) as f:\n",
    "    DATASET = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(DATASET)} examples\")\n",
    "print(f\"Sample: {DATASET[0]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "signature",
   "metadata": {},
   "source": "class MathReasoning(dspy.Signature):\n    \"\"\"Solve grade school math word problems step by step.\"\"\"\n    question: str = dspy.InputField(desc=\"A math word problem requiring multi-step reasoning\")\n    answer: str = dspy.OutputField(desc=\"The final numeric answer\")\n\n\n# Define metric as SOURCE STRING - bypasses inspect.getsource() issues in containers\nMETRIC_CODE = '''\ndef gsm8k_metric(gold, pred, trace=None, pred_name=None, pred_trace=None):\n    \"\"\"Score math prediction with feedback for GEPA reflection.\n\n    Args:\n        gold: Ground truth example with expected answer.\n        pred: Model prediction with generated answer.\n        trace: Optional execution trace.\n        pred_name: Optional predictor name.\n        pred_trace: Optional predictor trace.\n\n    Returns:\n        dspy.Prediction: Contains score (0.0-1.0) and feedback text.\n    \"\"\"\n    import re\n\n    def extract_number(text):\n        \"\"\"Extract the last number from text, handling commas.\"\"\"\n        if not text:\n            return \"\"\n        numbers = re.findall(r'-?[\\\\d,]+\\\\.?\\\\d*', text.replace(',', ''))\n        return numbers[-1] if numbers else text.strip()\n\n    expected = extract_number(gold.answer or \"\")\n    actual = extract_number(pred.answer or \"\")\n\n    if expected == actual:\n        return dspy.Prediction(score=1.0, feedback=\"Correct answer.\")\n    else:\n        feedback = \"Incorrect. Expected \" + str(gold.answer) + \", got \" + str(pred.answer) + \". Check each arithmetic step carefully.\"\n        return dspy.Prediction(score=0.0, feedback=feedback)\n'''\n\n# Execute to get callable for local testing\nexec(METRIC_CODE)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "serialize",
   "metadata": {},
   "source": "def serialize_source(obj: Any) -> str:\n    \"\"\"Extract source code from a function or DSPy Signature class.\n    \n    For metrics: define as string (METRIC_CODE = '''...''') and pass directly.\n    For Signatures: reconstructs from metadata automatically.\n    \n    Args:\n        obj: A function or dspy.Signature subclass.\n    \n    Returns:\n        str: Python source code as a string.\n    \n    Raises:\n        RuntimeError: If source cannot be extracted.\n    \"\"\"\n    # Handle DSPy Signatures via metadata reconstruction\n    if isinstance(obj, type) and issubclass(obj, dspy.Signature):\n        doc = obj.__doc__ or \"\"\n        lines = [\n            \"class \" + obj.__name__ + \"(dspy.Signature):\",\n            '    \"\"\"' + doc + '\"\"\"',\n        ]\n        for name, field in obj.model_fields.items():\n            extra = field.json_schema_extra or {}\n            ftype = \"InputField\" if extra.get(\"__dspy_field_type\") == \"input\" else \"OutputField\"\n            desc = extra.get(\"desc\", \"\")\n            lines.append(\"    \" + name + ': str = dspy.' + ftype + '(desc=\"' + desc + '\")')\n        return \"\\n\".join(lines)\n    \n    # For functions with __source_code__ attribute (from decorator)\n    if hasattr(obj, '__source_code__'):\n        return obj.__source_code__\n    \n    # Fallback to inspect (may fail in containers)\n    try:\n        return textwrap.dedent(inspect.getsource(obj)).strip()\n    except (OSError, TypeError) as e:\n        raise RuntimeError(\n            \"Cannot extract source from \" + str(obj) + \". \"\n            \"In containers, define metric as string: METRIC_CODE = '''def metric(...): ...'''\"\n        ) from e\n\n\nSIGNATURE_CODE = serialize_source(MathReasoning)\n# METRIC_CODE is already defined as a string above - no serialization needed!\n\nprint(\"Signature:\")\nprint(SIGNATURE_CODE)\nprint(\"\\nMetric:\")\nprint(METRIC_CODE)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "payload-header",
   "metadata": {},
   "source": [
    "## Build Payload\n",
    "\n",
    "Configure GEPA optimizer for prompt evolution on GSM8K math problems."
   ]
  },
  {
   "cell_type": "code",
   "id": "payload",
   "metadata": {},
   "source": [
    "payload = {\n",
    "    \"module_name\": \"dspy.ChainOfThought\",\n",
    "    \"signature_code\": SIGNATURE_CODE,\n",
    "    \"metric_code\": METRIC_CODE,\n",
    "    \"optimizer_name\": \"dspy.GEPA\",\n",
    "    \"optimizer_kwargs\": {\n",
    "        \"auto\": \"light\",\n",
    "        \"num_threads\": 8,\n",
    "        \"reflection_minibatch_size\": 3,\n",
    "    },\n",
    "    \"compile_kwargs\": {},\n",
    "    \"dataset\": DATASET,\n",
    "    \"column_mapping\": {\n",
    "        \"inputs\": {\"question\": \"question\"},\n",
    "        \"outputs\": {\"answer\": \"answer\"},\n",
    "    },\n",
    "    \"split_fractions\": {\"train\": 0.5, \"val\": 0.3, \"test\": 0.2},\n",
    "    \"shuffle\": True,\n",
    "    \"seed\": 42,\n",
    "    \"model_config\": MODEL_CONFIG,\n",
    "    \"reflection_model_config\": MODEL_CONFIG,\n",
    "}\n",
    "\n",
    "print(f\"Module: {payload['module_name']}\")\n",
    "print(f\"Optimizer: {payload['optimizer_name']}\")\n",
    "print(f\"Dataset: {len(payload['dataset'])} examples\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "submit-header",
   "metadata": {},
   "source": [
    "## Submit & Monitor Job"
   ]
  },
  {
   "cell_type": "code",
   "id": "submit",
   "metadata": {},
   "source": [
    "job_id = client.submit(payload)\n",
    "print(f\"Submitted job: {job_id}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "poll",
   "metadata": {},
   "source": "monitor = JobMonitor(client, job_id)\nresult = monitor.poll(interval=3)\n\nprint(\"\\nFinal status: \" + result[\"status\"])",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "results-header",
   "metadata": {},
   "source": [
    "## View Results"
   ]
  },
  {
   "cell_type": "code",
   "id": "results",
   "metadata": {},
   "source": [
    "def print_results(result: dict) -> None:\n",
    "    \"\"\"Print optimization results summary.\n",
    "    \n",
    "    Args:\n",
    "        result: Final job status dict from polling.\n",
    "    \"\"\"\n",
    "    if result[\"status\"] == \"success\":\n",
    "        r = result[\"result\"]\n",
    "        print(f\"Baseline score:  {r.get('baseline_test_metric', 'N/A')}\")\n",
    "        print(f\"Optimized score: {r.get('optimized_test_metric', 'N/A')}\")\n",
    "        print(f\"Runtime: {r.get('runtime_seconds', 0):.1f}s\")\n",
    "    else:\n",
    "        print(f\"Job failed: {result.get('message')}\")\n",
    "\n",
    "\n",
    "print_results(result)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "summary",
   "metadata": {},
   "source": [
    "client.summary(job_id)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "logs",
   "metadata": {},
   "source": [
    "def print_recent_logs(client: \"DSPyServiceClient\", job_id: str, n: int = 5) -> None:\n",
    "    \"\"\"Print the most recent log entries.\n",
    "    \n",
    "    Args:\n",
    "        client: DSPyServiceClient instance.\n",
    "        job_id: The job identifier.\n",
    "        n: Number of recent logs to display.\n",
    "    \n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    logs = client.logs(job_id)\n",
    "    print(f\"Total log entries: {len(logs)}\")\n",
    "    for log in logs[-n:]:\n",
    "        print(f\"  [{log['level']}] {log['message'][:80]}\")\n",
    "\n",
    "\n",
    "print_recent_logs(client, job_id)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "load-header",
   "metadata": {},
   "source": [
    "## Load & Test Optimized Program"
   ]
  },
  {
   "cell_type": "code",
   "id": "load",
   "metadata": {},
   "source": [
    "program = client.load_program(job_id)\n",
    "print(f\"Loaded program: {type(program).__name__}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "test",
   "metadata": {},
   "source": [
    "def test_program(program: dspy.Module, questions: list[str]) -> None:\n",
    "    \"\"\"Run test questions through the optimized program.\n",
    "    \n",
    "    Args:\n",
    "        program: The optimized DSPy module.\n",
    "        questions: List of test questions to run.\n",
    "    \"\"\"\n",
    "    for q in questions:\n",
    "        response = program(question=q)\n",
    "        print(f\"Q: {q}\")\n",
    "        print(f\"A: {response.answer}\\n\")\n",
    "\n",
    "\n",
    "test_questions = [\n",
    "    \"A bakery sells 24 cupcakes in the morning and 36 in the afternoon. If each cupcake costs $3, how much money did the bakery make?\",\n",
    "    \"Lisa has 48 stickers. She gives 1/4 of them to her friend and then buys 12 more. How many stickers does she have now?\",\n",
    "    \"A train travels at 60 mph for 2 hours, then at 80 mph for 1.5 hours. What is the total distance traveled?\",\n",
    "]\n",
    "\n",
    "test_program(program, test_questions)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "history",
   "metadata": {},
   "source": [
    "dspy.inspect_history(n=1)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}